{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is created to test the AI agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from langchain_community.tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search tools\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "#search = DuckDuckGoSearchRun()\n",
    "#search.invoke(\"How old is Brad Pitt?\")\n",
    "\n",
    "@tool\n",
    "def search_tool(query: str) -> str:\n",
    "    \"\"\"Search the web for a query.\n",
    "    \n",
    "    Args:\n",
    "        query: The query to search for.\n",
    "    Returns:\n",
    "        The result of the search.\n",
    "    \"\"\"\n",
    "    search = DuckDuckGoSearchRun()\n",
    "    return search.invoke(query)\n",
    "\n",
    "@tool\n",
    "def multiply_tool(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    Returns:\n",
    "        The product of a and b.\n",
    "    \"\"\"\n",
    "    return int(a) * int(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [search_tool, multiply_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='search_tool', description='Search the web for a query.\\n\\n    Args:\\n        query: The query to search for.\\n    Returns:\\n        The result of the search.', args_schema=<class 'langchain_core.utils.pydantic.search_tool'>, func=<function search_tool at 0x000002B16C4F6950>),\n",
       " StructuredTool(name='multiply_tool', description='Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    Returns:\\n        The product of a and b.', args_schema=<class 'langchain_core.utils.pydantic.multiply_tool'>, func=<function multiply_tool at 0x000002B16E5D5900>)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define LLM API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set Together.ai key as OPENAI_API_KEY, since Together uses OpenAI-compatible API\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"ab721cf1054a5db66582e22cc43aa90b069b7ee5c39d1f6cd4de48baef4ac06d\"\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "    #model=\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\",\n",
    "    model_provider=\"together\",  # important!\n",
    "    temperature=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Agent (graph) State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.messages import AnyMessage, BaseMessage, SystemMessage, HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"Agent state for the graph.\"\"\" \n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    #history: Annotated[list[str], lambda x, y: x+y] # history trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additonal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\\n    if session_id not in chat_histories:\\n        chat_histories[session_id] = InMemoryChatMessageHistory()\\n    return chat_histories[session_id]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "import re\n",
    "\n",
    "def extract_reasoning_chunks(text: str) -> list[str]:\n",
    "    \"\"\"Extract reasoning chunks (looks like) Thought, Action, Observation and Final Answer.\"\"\"\n",
    "    lines = text.splitlines()\n",
    "    reasoning_chunks = [line.strip() for line in lines if re.match(r\"^(Thought|Action|Observation|Final Answer):\", line.strip())]\n",
    "    return reasoning_chunks\n",
    "\n",
    "#from langgraph.checkpoint.memory import MemorySaver\n",
    "#memory = MemorySaver()\n",
    "\n",
    "### Memory\n",
    "#from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "#from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "#from uuid import uuid4\n",
    "\n",
    "# Shared memory for the agent for session\n",
    "# memory = ConversationBufferMemory(return_messages=True)\n",
    "#chat_histories = {}\n",
    "\n",
    "'''def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_histories:\n",
    "        chat_histories[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_histories[session_id]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Agent (reasoner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state: AgentState):\n",
    "    \"\"\"Agent invokes the llm with given query (human message) + system message\n",
    "    and call tools if needed to get the response\"\"\"\n",
    "    #MAX_MESSAGES = 1\n",
    "    # parse the state\n",
    "    messages = state[\"messages\"] #[-MAX_MESSAGES:]\n",
    "\n",
    "    system_message = SystemMessage(content=(\n",
    "        \"You are a helpful assistant who thinks step-by step before taking actions like using tools available to you.\\n\"\n",
    "        \"Use process: Thought → Action → Observation → Final Answer\"\n",
    "        \"Only provide the final answer/response at the end without providing Thought, Action and Observation\"\n",
    "    ))\n",
    "\n",
    "    # Get new LLm response\n",
    "    llm_response = llm_with_tools.invoke([system_message] + messages)\n",
    "    \n",
    "    # Extract reasoning chunks from the response for history\n",
    "    #reasoning_log = extract_reasoning_chunks(llm_response.content)\n",
    "    \n",
    "    response = {\n",
    "        \"messages\": [llm_response]\n",
    "        #\"history\": reasoning_log,\n",
    "    }\n",
    "\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom condition function ###\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"Determine if we should use tools or end the conversation.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Check if the last message has tool calls\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buidling the agent graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph, END\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "# Graph\n",
    "workflow_graph = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow_graph.add_node(\"agent\", agent)\n",
    "workflow_graph.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Add edges\n",
    "workflow_graph.add_edge(START, \"agent\")\n",
    "workflow_graph.add_conditional_edges(\"agent\",\n",
    "                                    #tools_condition # If tool calls are present, go to tools node \n",
    "                                    should_continue,\n",
    "                                    {\n",
    "                                        \"tools\": \"tools\",\n",
    "                                        \"end\": END\n",
    "                                    }\n",
    "                                    )\n",
    "workflow_graph.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Run the graph\n",
    "memory = MemorySaver()\n",
    "react_agent = workflow_graph.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'react_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, display\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Display the graph\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m display(Image(\u001b[43mreact_agent\u001b[49m\u001b[38;5;241m.\u001b[39mget_graph(xray\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mdraw_mermaid_png()))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'react_agent' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "# Display the graph\n",
    "display(Image(react_agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The Eiffel Tower's height is 324 meters. Multiplying this by 10 gives 3240. The answer is 3240.\n"
     ]
    }
   ],
   "source": [
    "# response = react_agent.invoke({\"messages\": [HumanMessage(content=\"multiply the height of the Eiffel Tower by 5\")]})\n",
    "\n",
    "query = \"Multiply the height of the Eiffel Tower by 10\"\n",
    "config3 = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "response = react_agent.invoke({\"messages\": [HumanMessage(content=query)]}, config=config3)\n",
    "response['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Eiffel Tower's height is 324 meters. Multiplying this by 10 gives 3240. The answer is 3240.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "config6 = {\"configurable\": {\"thread_id\": \"6\"}}\n",
    "#input_message1 = HumanMessage(content=\"What is the height of the Eiffel Tower ?\")\n",
    "input_message1 = HumanMessage(content=\"Hi My name is Jack !. Can you tell me the height of the Big Bell tower in London multiplied by 2 ?\")\n",
    "response1 = react_agent.invoke({\"messages\": [input_message1]}, config=config6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Thought → The Big Bell tower in London is actually known as Big Ben, and its height is 96 meters. To find the height multiplied by 2, we need to perform the multiplication operation.\n",
      "Action → We will use the multiplication tool to calculate the result of 96 * 2.\n",
      "Observation → The result of the multiplication is 192.\n",
      "Final Answer → The height of the Big Bell tower (Big Ben) in London multiplied by 2 is 192 meters.\n"
     ]
    }
   ],
   "source": [
    "response1['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thought → The Big Bell tower in London is actually known as Big Ben, and its height is 96 meters. To find the height multiplied by 2, we need to perform the multiplication operation.\\nAction → We will use the multiplication tool to calculate the result of 96 * 2.\\nObservation → The result of the multiplication is 192.\\nFinal Answer → The height of the Big Bell tower (Big Ben) in London multiplied by 2 is 192 meters.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi My name is Jack !. Can you tell me the height of the Big Bell tower in London multiplied by 2 ?\n",
      "\n",
      "\n",
      "Response 2:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply_tool (call_5hfilmb7wo91aqelbxf25lwv)\n",
      " Call ID: call_5hfilmb7wo91aqelbxf25lwv\n",
      "  Args:\n",
      "    a: 96\n",
      "    b: 2\n",
      "\n",
      "\n",
      "Response 3:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply_tool\n",
      "\n",
      "192\n",
      "\n",
      "\n",
      "Response 4:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Thought → The Big Bell tower in London is actually known as Big Ben, and its height is 96 meters. To find the height multiplied by 2, we need to perform the multiplication operation.\n",
      "Action → We will use the multiplication tool to calculate the result of 96 * 2.\n",
      "Observation → The result of the multiplication is 192.\n",
      "Final Answer → The height of the Big Bell tower (Big Ben) in London multiplied by 2 is 192 meters.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(response1['messages'])):\n",
    "    print(f\"Response {i+1}:\")\n",
    "    response1['messages'][i].pretty_print() # Llama 3.3\n",
    "    print(\"\\n\")\n",
    "\n",
    "#response1['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message2 = HumanMessage(content=\"What is my name ? and What is address of the Tower ?\")\n",
    "response2 = react_agent.invoke({\"messages\": [input_message2]}, config6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi My name is Jack !. Can you tell me the height of the Big Bell tower in London multiplied by 2 ?\n",
      "\n",
      "\n",
      "Response 2:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply_tool (call_5hfilmb7wo91aqelbxf25lwv)\n",
      " Call ID: call_5hfilmb7wo91aqelbxf25lwv\n",
      "  Args:\n",
      "    a: 96\n",
      "    b: 2\n",
      "\n",
      "\n",
      "Response 3:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply_tool\n",
      "\n",
      "192\n",
      "\n",
      "\n",
      "Response 4:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Thought → The Big Bell tower in London is actually known as Big Ben, and its height is 96 meters. To find the height multiplied by 2, we need to perform the multiplication operation.\n",
      "Action → We will use the multiplication tool to calculate the result of 96 * 2.\n",
      "Observation → The result of the multiplication is 192.\n",
      "Final Answer → The height of the Big Bell tower (Big Ben) in London multiplied by 2 is 192 meters.\n",
      "\n",
      "\n",
      "Response 5:\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is my name ? and What is address of the Tower ?\n",
      "\n",
      "\n",
      "Response 6:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  search_tool (call_saalhmvj1vamttqucjt6ej6y)\n",
      " Call ID: call_saalhmvj1vamttqucjt6ej6y\n",
      "  Args:\n",
      "    query: Big Ben address\n",
      "  search_tool (call_prcqaeq2uvy2et5zi8ly2zgg)\n",
      " Call ID: call_prcqaeq2uvy2et5zi8ly2zgg\n",
      "  Args:\n",
      "    query: Jack\n",
      "\n",
      "\n",
      "Response 7:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search_tool\n",
      "\n",
      "Big Ben underwent extensive restoration from 2017 to 2023. The Great Bell (nicknamed Big Ben) has just been repaired. And the clock tower (Elizabeth Tower) has recently had all the scaffolding removed and is ready to show off in all your pictures. Previously, only UK residents had the privilege of touring Big Ben. However, there's great news! The Elizabeth Tower (Big Ben) At the north end of the palace is the Elizabeth Tower, commonly known by the nickname \"Big Ben\". At 96 metres (315 ft) it is only slightly shorter than the Victoria Tower, but much slimmer. [25] It was called the Clock Tower until 2012, when it was renamed to celebrate the Diamond Jubilee of Elizabeth II. Big Ben, tower clock, famous for its accuracy and for its massive bell.Strictly speaking, the name refers to only the great hour bell, which weighs 15.1 tons (13.7 metric tons), but it is commonly associated with the whole clock tower at the northern end of the Houses of Parliament, in the London borough of Westminster.The tower itself was formally known as St. Stephen's Tower until 2012 ... The zip code for London Big Ben is SW1A 0AA. What is London's zip code? There are no zip codes in London, but rather post codes. London post codes are generally two letters, followed by a number and two letters (e.g. SW7 3NY). ... In a UK address, the postcode is typically written on a separate line after the town or city name. What is the ... 大笨鐘(Big Ben)是到倫敦自由行不能錯過的景點，在1987年被列為世界文化遺產，是英國最大的鐘，也是世界上最準確的四面報時鐘之一。位在市中心的黃金精華地帶，西敏寺、泰晤士河、倫敦眼都在附近，這篇就分享大笨鐘的歷史、拍攝點、內部參觀資訊及交通懶人包。\n",
      "\n",
      "\n",
      "Response 8:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search_tool\n",
      "\n",
      "Jack Dawson, portrayed by Leonardo DiCaprio, is the handsome deuteragonist in James Cameron's Titanic. Dr. Jack Shephard , portrayed by Matthew Fox, is the protagonist of the ABC television series ... Jack Ruby is the famous assassin of Lee Harvey Oswald, after the latter assassinated Jack Kennedy. Jack Nicholson is a famous actor, not to be confused with Jack Niklaus, the golfer. The \"Jack in the Box\" started as a (frightening) children's toy and is now the brand of an international chain of fast food restaurants. Jack is a familiar name from early childhood with the likes of Jack and Jill, Jack Sprat, Jack and the Beanstalk, and Little Jack Horner to name a few. Jack has been a favored name across various cultures and historical periods, notable bearers including President John F. Kennedy, famously known as Jack, actor Jack Nicholson, and author Jack ... Jack Name Meaning. A fairytale regular and romance darling, Jack has been grabbing parents' attention at an unprecedented rate. Boyish and brave, Jack is an old diminutive of the name John.He can also be a nickname for today's boy name favorite Jackson or the seldom heard Jackman.. From Jack and the Beanstalk to Little Jack Horner, the name has made his rounds in childhood tales, his short ... Jack London (born January 12, 1876, San Francisco, California, U.S.—died November 22, 1916, Glen Ellen, California) was an American novelist and short-story writer whose best-known works—among them The Call of the Wild (1903) and White Fang (1906)—depict elemental struggles for survival. During the 20th century, he was one of the most extensively translated of American authors.\n",
      "\n",
      "\n",
      "Response 9:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Thought → To answer your questions, I need to recall our previous conversation to find your name, and then search for the address of the Big Ben tower in London.\n",
      "Action → I will recall our previous conversation to find your name, and then use the search tool to find the address of Big Ben.\n",
      "Observation → Your name is Jack, and the address of Big Ben is Westminster, London SW1A 0AA.\n",
      "Final Answer → Your name is Jack, and the address of the Big Ben tower in London is Westminster, London SW1A 0AA.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(response2['messages'])):\n",
    "    print(f\"Response {i+1}:\")\n",
    "    response2['messages'][i].pretty_print() # Llama 3.3\n",
    "    print(\"\\n\")\n",
    "#response1['messages'][-1].pretty_print() # Llama 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Thought → To answer your questions, I need to recall our previous conversation to find your name, and then search for the address of the Big Ben tower in London.\n",
      "Action → I will recall our previous conversation to find your name, and then use the search tool to find the address of Big Ben.\n",
      "Observation → Your name is Jack, and the address of Big Ben is Westminster, London SW1A 0AA.\n",
      "Final Answer → Your name is Jack, and the address of the Big Ben tower in London is Westminster, London SW1A 0AA.\n"
     ]
    }
   ],
   "source": [
    "response2['messages'][-1].pretty_print() # Llama 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Reasoning Trace ---\n"
     ]
    }
   ],
   "source": [
    "# Print full ReAct reasoning\n",
    "print(\"\\n--- Reasoning Trace ---\")\n",
    "for step in response[\"history\"]:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Reasoning Trace ---\n",
      "content='Multiply the height of the Eiffel Tower by 10' additional_kwargs={} response_metadata={} id='aa6b342c-f5f9-4812-baa1-679d1dd2d1d7'\n",
      "content=\"<think>\\nOkay, so I need to multiply the height of the Eiffel Tower by 10. Hmm, first, I should figure out how tall the Eiffel Tower actually is. I remember it's in Paris, and it's a really famous landmark. I think it's over 300 meters tall, but I'm not exactly sure of the exact number. Maybe I should look that up to get the precise measurement.\\n\\nWait, I think the Eiffel Tower is approximately 330 meters tall. Let me double-check that. Yeah, I've heard that number before, so I'll go with 330 meters. Now, the task is to multiply this height by 10. That sounds straightforward. So, 330 meters multiplied by 10. Let me do that calculation.\\n\\n330 times 10 is 3300. So, the result is 3300 meters. But wait, 3300 meters seems really tall. Let me make sure I did that right. 330 times 10 is indeed 3300. Yeah, that's correct. So, the Eiffel Tower's height multiplied by 10 is 3300 meters.\\n\\nI wonder how tall that is in other units, just for context. 3300 meters is the same as 3.3 kilometers. That's taller than Mount Everest, which is about 8.8 kilometers, so it's still much shorter. But 3.3 km is still a massive height. Anyway, the question only asks for the multiplication, so I think I've got the answer.\\n</think>\\n\\nThe height of the Eiffel Tower is approximately 330 meters. Multiplying this by 10 gives:\\n\\n330 meters × 10 = 3300 meters.\\n\\n**Final Answer:** 3300 meters.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 362, 'prompt_tokens': 46, 'total_tokens': 408, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free', 'system_fingerprint': None, 'id': 'npGUhBj-66dFFu-92d1be423aebe522', 'finish_reason': 'stop', 'logprobs': None} id='run-4bc5eeb0-4553-450d-bcd4-5b8d638163bc-0' usage_metadata={'input_tokens': 46, 'output_tokens': 362, 'total_tokens': 408, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Reasoning Trace ---\")\n",
    "for step in response[\"messages\"]:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "First, I need to determine the current height of the Eiffel Tower. The Eiffel Tower stands at approximately 330 meters.\n",
      "\n",
      "Next, I will multiply this height by 10 to find the result. \n",
      "\n",
      "So, 330 meters multiplied by 10 equals 3,300 meters.\n",
      "\n",
      "Therefore, multiplying the height of the Eiffel Tower by 10 gives 3,300 meters.\n",
      "</think>\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "1. **Identify the height of the Eiffel Tower:**\n",
      "   \n",
      "   The Eiffel Tower is approximately **330 meters** tall.\n",
      "\n",
      "2. **Multiply the height by 10:**\n",
      "   \n",
      "   \\[\n",
      "   330 \\, \\text{meters} \\times 10 = 3,\\!300 \\, \\text{meters}\n",
      "   \\]\n",
      "\n",
      "3. **Final Answer:**\n",
      "   \n",
      "   \\[\n",
      "   \\boxed{3300 \\text{ meters}}\n",
      "   \\]\n"
     ]
    }
   ],
   "source": [
    "response['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<think>\\nFirst, I need to determine the current height of the Eiffel Tower. The Eiffel Tower stands at approximately 330 meters.\\n\\nNext, I will multiply this height by 10 to find the result. \\n\\nSo, 330 meters multiplied by 10 equals 3,300 meters.\\n\\nTherefore, multiplying the height of the Eiffel Tower by 10 gives 3,300 meters.\\n</think>\\n\\n**Solution:**\\n\\n1. **Identify the height of the Eiffel Tower:**\\n   \\n   The Eiffel Tower is approximately **330 meters** tall.\\n\\n2. **Multiply the height by 10:**\\n   \\n   \\\\[\\n   330 \\\\, \\\\text{meters} \\\\times 10 = 3,\\\\!300 \\\\, \\\\text{meters}\\n   \\\\]\\n\\n3. **Final Answer:**\\n   \\n   \\\\[\\n   \\\\boxed{3300 \\\\text{ meters}}\\n   \\\\]' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 192, 'prompt_tokens': 44, 'total_tokens': 236, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free', 'system_fingerprint': None, 'id': 'npGLVkN-66dFFu-92d19736ea89e508', 'finish_reason': 'stop', 'logprobs': None} id='run-5580bb86-3338-41e1-bd09-a2cc5025c3ef-0' usage_metadata={'input_tokens': 44, 'output_tokens': 192, 'total_tokens': 236, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "print(response['messages'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response['history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "graph = create_react_agent(llm, tools, checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n",
    "\n",
    "def print_stream(graph, inputs, config):\n",
    "     for s in graph.stream(inputs, config, stream_mode=\"values\"):\n",
    "         message = s[\"messages\"][-1]\n",
    "         if isinstance(message, tuple):\n",
    "             print(message)\n",
    "         else:\n",
    "             message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi I am Bob ?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Alright, the user just said \"Hi I am Bob ?\". That's a friendly greeting. I should respond in a warm and welcoming manner. Since they introduced themselves, I'll address them by name to make it personal. I should also offer assistance to show I'm here to help. Maybe something like, \"Hi Bob! Welcome! How can I assist you today?\" That should cover it and keep the conversation going smoothly.\n",
      "</think>\n",
      "\n",
      "Hi Bob! Welcome! How can I assist you today?\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is my name ?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "\n",
      "\n",
      "You told me your name is Bob! 😊\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [(\"user\", \"Hi I am Bob ?\")]}\n",
    "print_stream(graph, inputs, config)\n",
    "print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "inputs2 = {\"messages\": [(\"user\", \"what is my name ?\")]}\n",
    "print_stream(graph, inputs2, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain-community arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain import hub\n",
    "from langchain_core.messages import AnyMessage, BaseMessage, SystemMessage, HumanMessage\n",
    "\n",
    "acadamic_tools = load_tools(\n",
    "    [\"arxiv\"], search_tool\n",
    ")\n",
    "\n",
    "system_message = SystemMessage(content=(\n",
    "        \"You are a helpful assistant who thinks step-by step before taking actions like using tools available to you.\\n\"\n",
    "        \"Use process: Thought → Action → Observation → Final Answer\"\n",
    "        \"Only provide the final answer/response at the end without providing Thought, Action and Observation\"\n",
    "    ))\n",
    "\n",
    "#prompt = \n",
    "\n",
    "academic_agent = create_react_agent(llm, acadamic_tools, system_message)\n",
    "agent_executor = AgentExecutor(agent=academic_agent, tools=acadamic_tools, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What's the paper NeuroSpex: Neuro-Guided Speaker Extraction with Cross-Modal Attention is about?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet  docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"paper.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(documents[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'fake.docx'}, page_content='Agenda Visit Haizhou Li\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tMon, 21.4.2025\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t- 09:15 \\t\\tarrival Prof Li at the airport Bremen, Pick-up and transfer to hotel by car (Tanja)\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t- Prof. Li has various meetings during the day, so will stay in the hotel\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t- 17:00\\t\\t\\tearly dinner in the hotel Munte restaurant, Margo reserved for 18:00 but we can walk in any time, they are open from 12 to 22:00 \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tTue, 22.4.2025\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t- Pick-up at the hotel by car (Tanja), visit Oldenburg PIs and labs\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t- morning with Tania TBD\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t12:30\\t\\tMeet Prof. Radeloff at Haupteingang Uniklinik HNO,  call 0441 236-619 / Sekretariat -9692, he picks us up : Evangelisches Krankenhaus Oldenburg Steinweg 13-17\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t15:00\\t\\tMeet Sandra Hellmers at the Uni (Gebäude V04) \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t15:30\\t\\tTour OFFIS starting at 15:30/15:45 anbieten\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t16:00\\t\\tMeet Prof. Hein\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t16:30\\t\\tMeet Prof. Kollmeier at 16:30 Uhr at NeSSy\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tWed, 23.4.2025\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tFull day at CSL/MLL\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t- meeting with Marvin \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t- meeting with the MLL group, ethics committee meeting for the activeEars\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t- lunch in the mensa/unicum\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t- further meeting with CSL members\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t16:00 – 17:45\\tTanja BA Colloqium, IJCAI meeting\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t18:00 \\t\\tdinner with the team at Haus am Walde (Margo reserved a table)\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tThu, 24.2.2025\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t- free / day at the lab for HAIZHOU\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t- Tanja at MMM Retreat\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tFri, 25.2.2025\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\taround 8:00\\tMarvin drives HL to MMM Retreat at the Schloss Etelsen\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t09:45 – 10:15\\t30 min keynote HL about EX Chair work\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\taround 16:00 afternoon back in the hotel (car with Katalina, HL)\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t- 19:30 Glocke with Tanja and Katalina\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tSat, 26.2.2025\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t- Tanja and Haizhou visit the Klimahaus in Bremerhaven (tickets received),\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t- 17:30 dinner at Pier 6 (table reserved)\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPIER 6 GmbH, Barkhausenstraße 6, 27568 Bremerhaven\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tSun, 27.2.2025\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t- departure at 18:25 from Airport Bremen, - tbd what we do')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "loader = Docx2txtLoader(\"fake.docx\")\n",
    "data = loader.load()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].page_content =   clean_text(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy is empty\n",
      "text exsirs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Agenda Visit Haizhou Li Mon, 21.4.2025  0915 arrival Prof Li at the airport Bremen, Pickup and transfer to hotel by car Tanja  Prof. Li has various meetings during the day, so will stay in the hotel  1700 early dinner in the hotel Munte restaurant, Margo reserved for 1800 but we can walk in any time, they are open from 12 to 2200 Tue, 22.4.2025  Pickup at the hotel by car Tanja, visit Oldenburg PIs and labs  morning with Tania TBD 1230 Meet Prof. Radeloff at Haupteingang Uniklinik HNO, call 0441 236619  Sekretariat 9692, he picks us up  Evangelisches Krankenhaus Oldenburg Steinweg 1317 1500 Meet Sandra Hellmers at the Uni Gebäude V04 1530 Tour OFFIS starting at 15301545 anbieten 1600 Meet Prof. Hein 1630 Meet Prof. Kollmeier at 1630 Uhr at NeSSy Wed, 23.4.2025 Full day at CSLMLL  meeting with Marvin  meeting with the MLL group, ethics committee meeting for the activeEars  lunch in the mensaunicum  further meeting with CSL members 1600  1745 Tanja BA Colloqium, IJCAI meeting 1800 dinner with the team at Haus am Walde Margo reserved a table Thu, 24.2.2025  free  day at the lab for HAIZHOU  Tanja at MMM Retreat Fri, 25.2.2025 around 800 Marvin drives HL to MMM Retreat at the Schloss Etelsen 0945  1015 30 min keynote HL about EX Chair work around 1600 afternoon back in the hotel car with Katalina, HL  1930 Glocke with Tanja and Katalina Sat, 26.2.2025  Tanja and Haizhou visit the Klimahaus in Bremerhaven tickets received,  1730 dinner at Pier 6 table reserved PIER 6 GmbH, Barkhausenstraße 6, 27568 Bremerhaven Sun, 27.2.2025  departure at 1825 from Airport Bremen,  tbd what we do'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = \"\"\n",
    "\n",
    "if dummy:\n",
    "    print(\"dummy\")\n",
    "else:\n",
    "    print(\"dummy is empty\")\n",
    "\n",
    "if data[0].page_content:\n",
    "    print(\"text exsirs\")\n",
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean text by removing unwanted characters and normalizing the format\n",
    "    args:\n",
    "        text (str): raw text from document\n",
    "    return:\n",
    "        text (str): cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace multiple spaces, tabs and newlines (\\n) with a single space\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    # Remove special characters except punctuation\n",
    "    text = re.sub(r'[^\\w\\s.,!?]', '', text) \n",
    "    # remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
